| 讲义页码       | 配套实验        | 简介                          |
|------------|--------------------|-----------------------------------------|
| Page 6~8   | Blog [Hands-On Guide to LLM Decoding Strategies with ERNIE 4.5 — Implement Beam Search, Top-K Sampling & More in Real Code](https://medium.com/@alex_paddleocr/hands-on-guide-to-llm-decoding-strategies-with-ernie-4-5-d35bb90e1c80) and [Colab](https://colab.research.google.com/drive/1BVUNRkQTRskQiMoZAbcuJE_EO5pOXX5F?usp=sharing) | 介绍LLM的四种解码策略，以及在什么场景适用 |
| Page 12    | [从零开始组建并训练一个字符级RNN模型](https://aistudio.baidu.com/projectdetail/9511180)                                                                                                                     | 为了跟Transformer-based的LLM生成文字任务对应，在小型语料《了不起的盖茨比》上，以预测下一个字符的方式训练了一个经典的RNN模型，并展示RNN的局限性：不能捕捉长距离依赖，串行计算无法充分利用GPU。 |
| Page 12    | [从零开始组建并训练一个字符级LSTM模型](https://aistudio.baidu.com/projectdetail/9518356)                                                                                                                                                | LSTM通过门控机制控制信息流动，解决了传统RNN的梯度消失/爆炸问题，提升了长距离捕获依赖关系的能力 |
| Page 12    | [从零开始组建并训练一个字符级RNN with Attention模型](https://aistudio.baidu.com/projectdetail/9521997)                                                                                                                                                | 传统的编码器-解码器模型在处理长序列时存在信息瓶颈问题——整个输入序列被压缩成一个固定长度的向量，导致信息丢失。Bahdanau Attention通过动态关注输入序列的不同部分，解决了这个问题。RNN+Attention能够有效学习字符间的复杂依赖关系，生成更加连贯和有意义的文本！ |
| Assignment 1~2 | https://huggingface.co/spaces/AlexTransformer/ernie-4.5-logprob-compare                                                                                                                                   |总对数概率：单个词条对数概率之和, 衡量整个序列中模型的整体置信度。基础模型可能更擅长处理通用语言，指令调优模型可能更擅长处理特定任务，不同模型对不同类型的文本有不同的优势  |
